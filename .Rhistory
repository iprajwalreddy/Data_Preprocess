studentDataset_test<-studentDataset[701:1044,]
dim(studentDataset_test)
###############
#Model Develpoment:
library(caret)
ctrl <- trainControl(method = "repeatedcv",number = 10,repeats = 3)
grid <- expand.grid(C=c(0.01,0.05,0.1,0.25,0.5,0.75,1,1.25,1.5,1.75,2,5))
#train(target ~ . , method = Linear/Radial/Poly,grid,ctrl)
svmLinear_grade <- train(FinalGrade ~. -FinalGrade,studentDataset_train[-c(28,27)],method = "svmLinear",tuneGrid = grid, trainControl = ctrl)
#train(target ~ . , method = Linear/Radial/Poly,grid,ctrl)
svmLinear_grade <- train(FinalGrade ~. -FinalGrade,studentDataset_train[-c(28,27)],method = "svmLinear",tuneGrid = grid, trainControl = ctrl)
#train(target ~ . , method = Linear/Radial/Poly,grid,ctrl)
svmLinear_grade <- train(FinalGrade ~. -FinalGrade,studentDataset_train[-c(28,27)],method = "svmLinear",tuneGrid = grid, trainControl = ctrl)
rm(list = ls())
temp1<-tempfile()
download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/00356/student.zip",temp1)
unzip(temp1,"student-mat.csv")
studentData1<-read.csv("student-mat.csv",sep=";",header=T)
#studentData<-read.csv2("student-mat.csv")
View(studentData)
sum(is.na(studentData1))
unzip(temp1,"student-por.csv")
studentData2<-read.csv2("student-por.csv")
View(studentData2)
studentDataset<-rbind(studentData1,studentData2)
dim(studentDataset)
?prodNA()
#library(missForest)
#install.packages("missForest")
#studentDataset<-prodNA(studentDataset,noNA=0.1)
#sum(is.na(studentDataset))
######################################
??avg
studentDataset$prevScore<-(studentDataset$G1+studentDataset$G2)/2
str(studentDataset)
#Feature Engineering
?subset
studentDataset<-subset(studentDataset,select=-c(address,famsize,
nursery,Walc,G1,G2))
str(studentDataset)
colnames(studentDataset)[colnames(studentDataset)=="G3"] <- "finalScore"
str(studentDataset)
View(studentDataset)
studentDataset$FinalGrade<-factor(ifelse(studentDataset$finalScore>=median(studentDataset$finalScore),"PASS","FAIL"))
str(studentDataset)
###############
#Imputing Null Values
library(DT)
datatable(head(studentDataset))
sum(is.na(studentDataset$prevScore))
studentDataset$prevScore<-ifelse(is.na(studentDataset$prevScore),0,
studentDataset$prevScore)
studentDataset_imputed<-missForest(studentDataset)
studentDataset<-studentDataset_imputed$ximp
View(studentDataset)
ctrl <- trainControl(method = "repeatedcv",number = 10,repeats = 3)
grid <- expand.grid(C=c(0.01,0.05,0.1,0.25,0.5,0.75,1,1.25,1.5,1.75,2,5))
#train(target ~ . , method = Linear/Radial/Poly,grid,ctrl)
svmLinear_grade <- train(FinalGrade ~. -FinalGrade,studentDataset_train[-c(28,27)],method = "svmLinear",tuneGrid = grid, trainControl = ctrl)
rm(list = ls())
rm(list = ls())
temp1<-tempfile()
download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/00356/student.zip",temp1)
unzip(temp1,"student-mat.csv")
studentData1<-read.csv("student-mat.csv",sep=";",header=T)
#studentData<-read.csv2("student-mat.csv")
View(studentData)
sum(is.na(studentData1))
unzip(temp1,"student-por.csv")
studentData2<-read.csv2("student-por.csv")
View(studentData2)
studentDataset<-rbind(studentData1,studentData2)
dim(studentDataset)
?prodNA()
#library(missForest)
#install.packages("missForest")
#studentDataset<-prodNA(studentDataset,noNA=0.1)
#sum(is.na(studentDataset))
######################################
??avg
studentDataset$prevScore<-(studentDataset$G1+studentDataset$G2)/2
str(studentDataset)
#Feature Engineering
?subset
studentDataset<-subset(studentDataset,select=-c(address,famsize,
nursery,Walc,G1,G2))
str(studentDataset)
colnames(studentDataset)[colnames(studentDataset)=="G3"] <- "finalScore"
str(studentDataset)
View(studentDataset)
studentDataset$FinalGrade<-factor(ifelse(studentDataset$finalScore>=median(studentDataset$finalScore),"PASS","FAIL"))
str(studentDataset)
###############
#Imputing Null Values
library(DT)
datatable(head(studentDataset))
sum(is.na(studentDataset$prevScore))
studentDataset$prevScore<-ifelse(is.na(studentDataset$prevScore),0,
studentDataset$prevScore)
studentDataset_imputed<-missForest(studentDataset)
studentDataset<-studentDataset_imputed$ximp
View(studentDataset)
##############################################################
#By Doing backward Elimination we came to know that the attributes such as address,Walc,Helth can be removed
step(logisticModel_rawData)
########
#Testing and Training Samples
str(studentDataset)
studentDataset_train<-studentDataset[1:700,]
dim(studentDataset_train)
studentDataset_test<-studentDataset[701:1044,]
dim(studentDataset_test)
#Plotting graphs
library(ggplot2)
str(studentDataset)
?ggplot2()
print(ggplot(studentDataset, aes(x=FinalGrade))+geom_bar()+facet_grid(.~sex)+ggtitle("Result of student by Gender of Applicant"))
print(ggplot(studentDataset,aes(x=FinalGrade)) + geom_bar()+facet_grid(.~goout)+ggtitle("result of student regarding the impact of going out with friends(1-5 people)"))
print(ggplot(studentDataset,aes(x=FinalGrade)) + geom_bar()+facet_grid(.~higher)+ggtitle("result of student prediction based on the higher education plans"))
#######################Logistic Regression###################
View(studentDataset_test)
LogisticModel_1 <- glm(FinalGrade ~ . -FinalGrade,studentDataset_train[-c(27,26)],family = "binomial")
summary(LogisticModel_1)
LogisticPredict_1 <- predict(LogisticModel_1,studentDataset_test[-c(27,26)],type = "response")
summary(LogisticPredict_1)
#Here we have considered threshold value of 0.5
table(Actualvalue = studentDataset_test$FinalGrade,PredictedValue=LogisticPredict_1 >0.5)#72%
#How to find the Threshold -> ROCR
#For this we will take train data to know the threshold for that and apply for test data
predict_train <- predict(LogisticModel_1,studentDataset_train,type= "response")
install.packages("ROCR")
library(ROCR)
ROCRPrediction <- prediction(predict_train,studentDataset_train$FinalGrade)
ROCRPerformance <- performance(ROCRPrediction,"tpr","fpr")
plot(ROCRPerformance,print.cutoffs.at=seq(0.1,by=0.1))
#Here we have considered threshold value of 0.4 by looking into the ROCR graph
table(Actualvalue = studentDataset_test$FinalGrade,PredictedValue=LogisticPredict_1 >0.4)#75%
#########################
library(caret)
library(rpart)
install.packages("C50")
library(C50)
install.packages("lava")
#decision tree
model_Tree <- C5.0(FinalGrade ~ . -FinalGrade,studentDataset_train[-c(27,26)],rules = FALSE)
?predict()
prediction <- predict(model_Tree,studentDataset_test)
str(prediction)
summary(prediction)
View(prediction)
#prediction2 <- predict(model_Tree,studentDataset_test[702,])
#prediction2
install.packages("gmodel")
install.packages("FSelector")
library("rJava")
library("FSelector")
install.packages("rJava")
Sys.setenv(JAVA_HOME = "C:/Program Files/Java/jdk1.8.0_101/")
if(Sys.getenv("JAVA_HOME")!=""){
Sys.setenv(JAVA_HOME="")
}
library(rJava)
install.packages("e1071")
library(e1071)
library(lava)
library(caret)
?confusionMatrix()
confusionMatrix(data = prediction,reference = studentDataset_test$FinalGrade)
library(caret)
ctrl <- trainControl(method = "repeatedcv",number = 10,repeats = 3)
grid <- expand.grid(C=c(0.01,0.05,0.1,0.25,0.5,0.75,1,1.25,1.5,1.75,2,5))
#train(target ~ . , method = Linear/Radial/Poly,grid,ctrl)
svmLinear_grade <- train(FinalGrade ~. -FinalGrade,studentDataset_train[-c(28,27)],method = "svmLinear",tuneGrid = grid, trainControl = ctrl)
install.packages("lava")
install.packages("lava")
install.packages("lava")
install.packages("lava")
install.packages("lava")
install.packages("lava")
install.packages("lava")
install.packages("lava")
#train(target ~ . , method = Linear/Radial/Poly,grid,ctrl)
svmLinear_grade <- train(FinalGrade ~. -FinalGrade,studentDataset_train[-c(28,27)],method = "svmLinear",tuneGrid = grid, trainControl = ctrl)
library(caret)
#train(target ~ . , method = Linear/Radial/Poly,grid,ctrl)
svmLinear_grade <- train(FinalGrade ~. -FinalGrade,studentDataset_train[-c(28,27)],method = "svmLinear",tuneGrid = grid, trainControl = ctrl)
svmLinear_grade
?train()
grid_radial <- expand.grid(sigma = c(0.01,0,0.015,0.2),C=(0.01,0.05,0.1,0.25,0.5,0.75,1,1.25,1.5,1.75,2,5))
grid_radial <- expand.grid(sigma = c(0.01,0,0.15,0.2), C=(0.01,0.05,0.1,0.25,0.5,0.75,1,1.25,1.5,1.75,2,5))
grid_radial <- expand.grid(sigma = c(0.01,0,0.15,0.2), C= c(0.01,0.05,0.1,0.25,0.5,0.75,1,1.25,1.5,1.75,2,5))
#train(target ~ . , method = Linear/Radial/Poly,grid,ctrl)
svmLinear_grade <- train(FinalGrade ~. -FinalGrade,studentDataset_train[-c(28,27)],method = "svmLinear",tuneGrid = grid_sigma, trainControl = ctrl)
#train(target ~ . , method = Linear/Radial/Poly,grid,ctrl)
svmLinear_grade <- train(FinalGrade ~. -FinalGrade,studentDataset_train[-c(28,27)],method = "svmLinear",tuneGrid = grid_radial, trainControl = ctrl)
grid_radial <- expand.grid(sigma = c(0.01,0,0.15,0.2),C= c(0.01,0.05,0.1,0.25,0.5,0.75,1,1.25,1.5,1.75,2,5))
#train(target ~ . , method = Linear/Radial/Poly,grid,ctrl)
svmLinear_grade <- train(FinalGrade ~. -FinalGrade,studentDataset_train[-c(28,27)],method = "svmLinear",tuneGrid = grid_radial, trainControl = ctrl)
#train(target ~ . , method = Linear/Radial/Poly,grid,ctrl)
svmLinear_grade <- train(FinalGrade ~. -FinalGrade,studentDataset_train[-c(28,27)],method = "svmLinear2",tuneGrid = grid_radial, trainControl = ctrl)
#train(target ~ . , method = Linear/Radial/Poly,grid,ctrl)
svmLinear_grade <- train(FinalGrade ~. -FinalGrade,studentDataset_train[-c(28,27)],method = "svmRadial",tuneGrid = grid_radial, trainControl = ctrl)
svmLinear_grade
#train(target ~ . , method = Linear/Radial/Poly,grid,ctrl)
svmLinear_grade <- train(FinalGrade ~. -FinalGrade,studentDataset_train[-c(28,27)],method = "svmLinear2",tuneGrid = grid, trainControl = ctrl)
ctrl <- trainControl(method = "repeatedcv",number = 10,repeats = 3)
grid <- expand.grid(C=c(0.01,0.05,0.1,0.25,0.5,0.75,1,1.25,1.5,1.75,2,5))
#train(target ~ . , method = Linear/Radial/Poly,grid,ctrl)
svmLinear_grade <- train(FinalGrade ~. -FinalGrade,studentDataset_train[-c(28,27)],method = "svmLinear2",tuneGrid = grid, trainControl = ctrl)
#train(target ~ . , method = Linear/Radial/Poly,grid,ctrl)
svmLinear_grade <- train(FinalGrade ~. -FinalGrade,studentDataset_train[-c(28,27)],method = "svmLinear2")
svmLinear_grade
grid_cost <- expand.grid(Cost=c(0.01,0.05,0.1,0.25,0.5,0.75,1,1.25,1.5,1.75,2,5))
#train(target ~ . , method = Linear/Radial/Poly,grid,ctrl)
svmLinear_grade <- train(FinalGrade ~. -FinalGrade,studentDataset_train[-c(28,27)],method = "svmLinear2",tuneGrid = grid_cost, trainControl = ctrl)
#train(target ~ . , method = Linear/Radial/Poly,grid,ctrl)
svmLinear_grade <- train(FinalGrade ~. -FinalGrade,studentDataset_train[-c(28,27)],method = "svmLinear2")
######Prediction
predicted_model <- predict(svmLinear_grade,studentDataset_test)
View(predicted_model)
####Confusion Matrix
Matrixs <- confusionMatrix(data = predicted_model,reference = studentDataset_test$FinalGrade)
Matrixs
#train(target ~ . , method = Linear/Radial/Poly,grid,ctrl)
svmLinear_grade2 <- train(FinalGrade ~. -FinalGrade,studentDataset_train[-c(28,27)],method = "svmLinear",tuneGrid = grid, trainControl = ctrl)
#######
result <- resamples(list(linear = svmLinear_grade2,Linear2 = svmLinear_grade))
bwplot(result)
?trainControl()
ctrl <- trainControl(method = "boot",number = 10,repeats = 3)
ctrl <- trainControl(method = "boot",number = 10)
ctrl <- trainControl(method = "repeatedcv",number = 10,repeats = 3)
ctrl2 <- trainControl(method = "boot",number = 10)
#train(target ~ . , method = Linear/Radial/Poly,grid,ctrl)
svmLinear_grade <- train(FinalGrade ~. -FinalGrade,studentDataset_train[-c(28,27)],method = "svmLinear",tuneGrid = grid,trainControl = ctrl2)
svmLinear_grade
######Prediction
predicted_model <- predict(svmLinear_grade,studentDataset_test)
####Confusion Matrix
Matrixs <- confusionMatrix(data = predicted_model,reference = studentDataset_test$FinalGrade)
Matrixs
#train(target ~ . , method = Linear/Radial/Poly,grid,ctrl)
svmLinear_grade3 <- train(FinalGrade ~. -FinalGrade,studentDataset_train[-c(28,27)],method = "svmLinear2")
#######
result <- resamples(list(linear = svmLinear_grade3,Linear2 = svmLinear_grade))
bwplot(result)
#######
result <- resamples(list(linear2 = svmLinear_grade3,Linear_boot = svmLinear_grade))
bwplot(result)
ctrl3 <- trainControl(method = "cv",number = 10)
ctrl3 <- trainControl(method = "cv",number = 10,repeats = 3)
ctrl3 <- trainControl(method = "cv",number = 10)
grid <- expand.grid(C=c(0.01,0.05,0.1,0.25,0.5,0.75,1,1.25,1.5,1.75,2,5))
#train(target ~ . , method = Linear/Radial/Poly,grid,ctrl)
svmLinear_grade4 <- train(FinalGrade ~. -FinalGrade,studentDataset_train[-c(28,27)],method = "svmLinear",tuneGrid = grid,trainControl = ctrl2)
svmLinear_grade
svmLinear_grade4
####Confusion Matrix
Matrixs <- confusionMatrix(data = predicted_model4,reference = studentDataset_test$FinalGrade)
Matrixs
#######
result <- resamples(list(linear2 = svmLinear_grade3,Linear_boot = svmLinear_grade,Linear_cv = svmLinear_grade4))
bwplot(result)
#######
result <- resamples(list(linear2 = svmLinear_grade3,Linear_boot = svmLinear_grade,Linear_cv = svmLinear_grade4,Linear_repeated = svmLinear_grade2))
bwplot(result)
##################
Repeated_Matrix <- confusionMatrix(data = svmLinear_grade2,reference = studentDataset_test$FinalGrade)
Boot_Matrix <- confusionMatrix(data = svmLinear_grade,reference = studentDataset_test$FinalGrade)
cv_Matrix <- confusionMatrix(data = svmLinear_grade4,reference = studentDataset_test$FinalGrade)
linear2_Matrix <- confusionMatrix(data = svmLinear_grade3,reference = studentDataset_test$FinalGrade)
result_Test <- resamples(list(repeated = Repeated_Matrix,boot = Boot_Matrix, cv = cv_Matrix,linear2 = linear2_Matrix))
# Code to build factor_survey_vector
survey_vector <- c("M", "F", "F", "M", "M")
factor_survey_vector <- factor(survey_vector)
# Specify the levels of factor_survey_vector
levels(factor_survey_vector) <- c("male","female")
factor_survey_vector
# Specify the levels of factor_survey_vector
levels(factor_survey_vector) <- c("female","male")
factor_survey_vector
# Code to build factor_survey_vector
survey_vector <- c("M", "F", "F", "M", "M")
factor_survey_vector <- factor(survey_vector)
levels(factor_survey_vector)
install.packages("mtcars")
mtcars
source('~/.active-rstudio-document', echo=TRUE)
rm(list = ls())
train_data <- read.csv("C:\\Users\\P Naresh Reddy\\Desktop\\allstate-purchase-prediction-challenge\\train.csv")
#My use case
rm(list = ls())
train_data <- read.csv("C:\\Users\\P Naresh Reddy\\Desktop\\allstate-purchase-prediction-challenge\\train.csv")
#My use case
rm(list = ls())
train_data <- read.csv("C:\\Users\\P Naresh Reddy\\Desktop\\allstate-purchase-prediction-challenge\\train.csv")
test_data <- read.csv("C:\\Users\\P Naresh Reddy\\Desktop\\allstate-purchase-prediction-challenge\\test_v2.csv")
#Checking the data
sum.isna(train_data)
#Checking the data
View(train_dat)
#Checking the data
View(train_data)
summary(train_data)
number <- sum(is.na(train_data))
number
dim(number)
dim(train_data)
#Checking the test data
Vies(test_data)
#Checking the test data
View(test_data)
summary(test_data)
number2 <- sum(is.na(test_data))
number2
dim(test_data)
#Checking the train data
View(train_data)
#Checking the train data
View(train_data)
tail(train_data)
#Checking the train data
View(train_data)
str(test_data)
str(train_data)
train_data$risk_factor <- if(train_data$risk_factor == NA,mean(train_data$risk_factor),train_data$risk_factor)
train_data$risk_factor <- if(train_data$risk_factor == NA, mean(train_data$risk_factor),train_data$risk_factor)
train_data$risk_factor <- if(train_data$risk_factor == NA, mean(train_data$risk_factor),train_data$risk_factor)
?if()
?ifelse()
train_data$risk_factor <- ifelse(train_data$risk_factor == NA mean(train_data$risk_factor),train_data$risk_factor)
train_data$risk_factor <- ifelse(train_data$risk_factor == NA,mean(train_data$risk_factor),train_data$risk_factor)
sum(is.na(train_data$risk_factor))
?mean()
library(corrplot)
?corrplot()
graph <- corrplot(corr,method = "number")
graph <- corrplot(train_data,method = "number")
source('~/.active-rstudio-document', echo=TRUE)
result <- two_dice()
result
print(possibilities)
possibilities <- 1:6
print(possibilities)
?sample()
possibilities <- 1:6
print(possibilities)
dice1 <- sample(possibilities, size = 1)
print(dice1)
dice2 <- sample(possibilities, size = 1)
print(dice2)
dice1 + dice2
result <- two_dice()
result
rm(list = ls())
count <- increment(count, 2)
rm(list = ls())
increment <- function(x, inc = 1){
x <- x + inc
x
}
count <- 5
a <- increment(count, 2)
b <- increment(count)
count <- increment(count, 2)
rm(list = ls())
temp1<-tempfile()
download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/00356/student.zip",temp1)
unzip(temp1,"student-mat.csv")
studentData1<-read.csv("student-mat.csv",sep=";",header=T)
#studentData<-read.csv2("student-mat.csv")
View(studentData1)
unzip(temp1,"student-por.csv")
studentData2<-read.csv2("student-por.csv")
studentDataset<-rbind(studentData1,studentData2)
dim(studentDataset)
studentDataset$prevScore<-(studentDataset$G1+studentDataset$G2)/2
#Feature Engineering
studentDataset<-subset(studentDataset,select=-c(address,famsize,
nursery,Walc,G1,G2))
str(studentDataset)
colnames(studentDataset)[colnames(studentDataset)=="G3"] <- "finalScore"
str(studentDataset)
View(studentDataset)
#Testing and Training Samples
str(studentDataset)
studentDataset_train<-studentDataset[1:700,]
dim(studentDataset_train)
studentDataset_test<-studentDataset[702:1044,]
dim(studentDataset_test)
library(C50)
c50model <- C5.0(studentDataset_train[-c(27,28,29)], studentDataset_train$FinalGrade)
c50model <- C5.0(studentDataset_train[-c(27,28,29)], studentDataset_train$FinalGrade)
c50predict <- predict(c50model, studentDataset_test[-c(27, 28, 29)])
c50model <- C5.0(studentDataset_train[-c(27,28,29)], studentDataset_train$FinalGrade)
studentDataset$FinalGrade<-factor(ifelse(studentDataset$finalScore>=median(studentDataset$finalScore),"PASS","FAIL"))
#Testing and Training Samples
str(studentDataset)
studentDataset_train<-studentDataset[1:700,]
dim(studentDataset_train)
studentDataset_test<-studentDataset[702:1044,]
dim(studentDataset_test)
library(C50)
c50model <- C5.0(studentDataset_train[-c(27,28,29)], studentDataset_train$FinalGrade)
c50predict <- predict(c50model, studentDataset_test[-c(27, 28, 29)])
library(gmodels)
CrossTable(c50predict,studentDataset_test$FinalGrade)
sum(c50predict == studentDataset_test$FinalGrade)/length(studentDataset_test$FinalGrade)
library(caret)
confusionMatrix(data = c50predict, reference = studentDataset_test$FinalGrade)
library(rJava)
library(rJava)
install.packages(rjava)
install.packages(rJava)
install.packages("rJava")
library(rJava)
library(FSelector)
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre7') # for 64-bit version
require(rjson)
library(rJava)
require(rjson)
install.packages("rjson")
require(rjson)
library(rJava)
require(rJava)
require(rJava)
library(rjson)
library(rJava)
rm(list = ls())
install.packages("rjson")
install.packages("rjson")
library("data.table")
library(c("data.table","rjson"))
train_data$risk_factor <- ifelse(is.na(train_data$risk_factor),mean(train_data$risk_factor),train_data$risk_factor)
sum(is.na(train_data$risk_factor))
rm(list =ls())
# importing the data
train_data <- read.csv("C:\\Users\\P Naresh Reddy\\Desktop\\allstate-purchase-prediction-challenge\\train.csv")
copy_data <- train_data
# Removing the unwanted data
train_data <- select(train_data, -c('C','D','E','F','G'))
library(dplyr)
# Removing the unwanted data
train_data <- select(train_data, -c('C','D','E','F','G'))
#Checking for NA's
sum(is.na(train_data))
train_data$risk_factor <- ifelse(is.na(train_data$risk_factor),mean(train_data$risk_factor),train_data$risk_factor)
sum(is.na(train_data$risk_factor))
train_data$C_previous <- ifelse(is.na(train_data$C_previous), mean(train_data$C_previous),train_data$C_previous)
train_data$risk_factor <- ifelse(is.na(train_data$risk_factor),mean(train_data$risk_factor),train_data$risk_factor,rm.na == TRUE)
View(copy_data)
View(copy_data)
View(train_data)
rm(list=ls())
#To remove all the previous data
rm(list = ls())
setwd("~/")
setwd("~/")
#Importing the data
dataset = read.csv('Data.csv')
#Importing the data
dataset = read.csv('Data.csv')
dataset = read.csv('Data.csv')
setwd("F:/ml/Machine Learning A-Z/Part 1 - Data Preprocessing/Data_Preprocess")
#Importing the data
dataset = read.csv('Data.csv')
View(dataset)
#Taking care of missing data
?ifelse()
dataset$age = ifelse(is.na(dataset$Age),ave(dataset$Age, FUN = function(X) mean(X, na.rm = TRUE)),dataset$Age))
dataset$age = ifelse(is.na(dataset$Age),ave(dataset$Age, FUN = function(X) mean(X, na.rm = TRUE)),dataset$Age)
View(dataset)
?ave()
dataset$age = ifelse(is.na(dataset$Age),ave(dataset$Age, FUN = function(x) mean(x, na.rm = TRUE)),dataset$Age)
View(dataset)
dataset$age = ifelse(is.na(dataset$Age),ave(dataset$Age, FUN = function(dataset$Age) mean(dataset$Age, na.rm = TRUE)),dataset$Age)
dataset$Age = ifelse(is.na(dataset$Age),
ave(dataset$Age, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Age)
View(dataset)
View(dataset)
dataset$Salary = ifelse(is.na(dataset$Salary),
ave(dataset$Salary, FUN = function(x) mean(x, na.rm =TRUE)),
dataset$Salary)
View(dataset)
View(dataset)
?factor()
dataset$Country = factor(dataset$Country, levels = c('France','Spain','Germany'),
labels = (1,2,3))
#Encoding the categorical data
dataset$Country = factor(dataset$Country, levels = c('France','Spain','Germany'),
labels = c(1,2,3))
View(dataset)
dataset$Purchased = factor(dataset$Purchased, levels = c('No','Yes'),
labels = c(0,1))
#Splitting the data into train and test models
install.packages('caTools')
library(caTools)
?set.seed()
set.seed(123)
split = sample.split(dataset,SplitRatio = 0.8)
training_set = subset(dataset,split = TRUE)
test_set = subset(dataset,split = FALSE)
View(test_set)
training_set = subset(dataset,split == TRUE)
test_set = subset(dataset,split == FALSE)
#Feature Scaling
training_set[:,2:3] = scale(training_set[:,2:3])
#Feature Scaling
training_set[,2:3] = scale(training_set[,2:3])
test_set[,2:3] = scale(test_set[,2:3])
View(training_set)
